<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GazeVal</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="#">David Wong*</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Bin Wang*</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Gorkem Durak</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Marouane Tliba</a><sup>2</sup>,</span>
            <span class="author-block"><a href="#">Akshay Chaudhari</a><sup>3</sup>,</span>
            <span class="author-block"><a href="#">Aladine Chetouani</a><sup>4</sup>,</span>
            <span class="author-block"><a href="#">Ahmet Enis Cetin</a><sup>2</sup>,</span>
            <span class="author-block"><a href="#">Cagdas Topel</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Nicolo Gennaro</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Camila Lopes Vendrami</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Tugce Agirlar Trabzonlu</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Amir Ali Rahsepar</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Laetitia Perronne</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Matthew Antalek</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Onural Ozturk</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Gokcan Okur</a><sup>5</sup>,</span>
            <span class="author-block"><a href="#">Andrew C. Gordon</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Ayis Pyrros</a><sup>6</sup>,</span>
            <span class="author-block"><a href="#">Frank H. Miller</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Amir Borhani</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Hatice Savas</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Eric Hart</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Drew Torigian</a><sup>7</sup>,</span>
            <span class="author-block"><a href="#">Jayaram K. Udupa</a><sup>7</sup>,</span>
            <span class="author-block"><a href="#">Elizabeth Krupinski</a><sup>8</sup>,</span>
            <span class="author-block"><a href="#">Ulas Bagci</a><sup>1</sup></span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Northwestern University,</span>
            <span class="author-block"><sup>2</sup>University of Illinois at Chicago,</span>
            <span class="author-block"><sup>3</sup>Stanford University,</span>
            <span class="author-block"><sup>4</sup>Universit√© Sorbonne Paris Nord,</span>
            <span class="author-block"><sup>5</sup>Loyola University Chicago,</span>
            <span class="author-block"><sup>6</sup>DuPage Medical Group,</span>
            <span class="author-block"><sup>7</sup>University of Pennsylvania,</span>
            <span class="author-block"><sup>8</sup>Emory University</span>
          </div>
          <div class="is-size-6"><em>* Equal contribution.</em></div>

          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.20967"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="image-container">
        <img class="img-responsive" src="static/images/framework.png" alt="Teaser Image">
      </div>
      <h2 class="subtitle has-text-centered">
        Overview of the proposed GazeVal framework, which introduces two tasks with corresponding evaluation metrics to quantitatively assess the quality of synthetic Chest X-ray images with expert knowledge.  
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The demand for high-quality synthetic data for model training and augmentation has never been greater in medical imaging. However, current evaluations predominantly rely on computational metrics that fail to align with human expert recognition. This leads to synthetic images that may appear realistic numerically but lack clinical authenticity, posing significant challenges in ensuring the reliability and effectiveness of AI-driven medical tools. To address this gap, we introduce GazeVal, a practical framework that synergizes expert eye-tracking data with direct radiological evaluations to assess the quality of synthetic medical images. GazeVal leverages gaze patterns of radiologists as they provide a deeper understanding of how experts perceive and interact with synthetic data in different tasks (i.e., diagnostic or Turing tests). Experiments with sixteen radiologists revealed that 96.6\% of the generated images (by the most recent state-of-the-art AI algorithm) were identified as fake, demonstrating the limitations of generative AI in producing clinically accurate images.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Methods</h2>

                    <br>
        <div class="subtitle has-text-justified">
          <p>
            Given a real chest X-ray image and its associated report, a synthetic chest X-ray image is generated based on the report by an LDM-based generative model, RoentGen. The synthetic images are then placed in a dataset with real images and reviewed by radiologists under two task settings. First, radiologists are asked to provide a diagnosis without knowing that synthetic images are included. Second, they are asked to determine whether each image is real or generated. At the same time, we use eye tracking to record their eye gaze during the tasks. Using the gaze data and their task answers, we can compare synthetic and real X-rays across various metrics to evaluate the quality of the synthetic images.
          </p>
        </div>
        <div class="image-container">
          <img class="img-responsive" src="static/images/setup.png" alt="one click">
        </div> <br>
              <div class="subtitle has-text-justified">
                <p>Left: Eye tracking setup. The radiologist is viewing the image on the monitor with the eye tracker in between them. Middle: EyeLink 1000 Plus eye-tracker view with calibration software. Right: Eye-tracker and example attention map.
                </p>
              </div>
             
            
          </div>
        </div>
</div></section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Results</h2>

                    <br>
        <div class="subtitle has-text-justified">
          <p>
            (1) This means the diagnostic agreement for real and synthetic images are similar.
          </p>
          <p>
            (2) Radiologists can accurately distinguish between real and synthetic images, which means the current state-of-the-art generative model are unable to generate highly realistic radiological images yet.
          </p>
        </div>
        <div class="image-container">
          <img class="img-responsive" src="static/images/Box_Whisker2.png" alt="one click">
        </div> <br>
      </div>
        </div>

        <div class="subtitle has-text-justified">
          <p>
          (3) Differing viewing patterns of real and synthetic X-rays are present and increase with prolonged visual processing.
          </p>
        </div>
        <div class="image-container">
          <img class="img-responsive" src="static/images/table.png" alt="one click">
        </div>
          </div>
        </div>

</div></section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <div class="subtitle has-text-justified">
          <p>
          (4) Using both Fixation-Based and Scanpath-Based Congruency, as well as shared attention calculations, we quantitatively determine that visual attention is task-guided.
          </p>
        </div>
        <div class="image-container">
          <img class="img-responsive" src="static/images/Table2.png" style="max-width: 100%; height: auto;" alt="one click">
        </div> <br>
        <div class="image-container">
          <img class="img-responsive" src="static/images/IoUFigure3.png" style="max-width: 100%; height: auto;" alt="one click">
        </div> <br>
          </div>
        </div>

        <div class="subtitle has-text-justified">
          <p>
          (5) Generative models struggle more to generate realistic X-rays when containing pathologies.
          </p>
        </div>
        <div class="image-container">
          <img class="img-responsive" src="static/images/pathology.png" alt="one click">
        </div>
          </div>
        </div>

</div></section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{wong2025eyestelltruthgazeval,
      title={Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging}, 
      author={David Wong and Bin Wang and Gorkem Durak and Marouane Tliba and Akshay Chaudhari and Aladine Chetouani and Ahmet Enis Cetin and Cagdas Topel and Nicolo Gennaro and Camila Lopes Vendrami and Tugce Agirlar Trabzonlu and Amir Ali Rahsepar and Laetitia Perronne and Matthew Antalek and Onural Ozturk and Gokcan Okur and Andrew C. Gordon and Ayis Pyrros and Frank H. Miller and Amir Borhani and Hatice Savas and Eric Hart and Drew Torigian and Jayaram K. Udupa and Elizabeth Krupinski and Ulas Bagci},
      year={2025},
      eprint={2503.20967},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.20967}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
