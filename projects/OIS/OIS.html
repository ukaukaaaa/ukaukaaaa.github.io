<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GazeSAM">
  <meta name="keywords" content="Interaction Image Segmentation, Interactive Segmentation, Order, Depth">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Order-aware Interactive Segmentation</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ut_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Order-aware Interactive Segmentation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ukaukaaaa.github.io">Bin Wang</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=KQnKUHKGFdUC&hl=en">Anwesa Choudhuri</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://mzhengrpi.github.io/">Meng Zheng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/gaozhongpai/home">Zhongpai Gao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://planche.me/">Benjamin Planche</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://dengandong.github.io/">Andong Deng</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/cs.unc.edu/qinliu/home?pli=1">Qin Liu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://bagcilab.com">Ulas Bagci</a><sup>1</sup>,
            </span>      
            <span class="author-block">
              <a href="https://wuziyan.com/">Ziyan Wu</a><sup>2</sup>
            </span>                                                                                                 
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Northwestern University</span>
            <span class="author-block"><sup>2</sup>United Imaging Intelligence</span> <br>
            <span class="author-block"><sup>3</sup>University of Central Florida</span>
            <span class="author-block"><sup>4</sup>University of North Carolina</span>
          </div>
<!-- 
          <div class="is-size-5 conference">

          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=hJ5DREWdjs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=xIUSG0pLNyo&t=119s"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
               <!--Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/ukaukaaaa/OIS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="image-container">
        <img class="img-responsive" src="teaser.png" alt="Teaser Image">
      </div>
      <h2 class="subtitle has-text-centered">
        Each object has a specific <span style="color: orange; font-weight:bold">order</span>, or relative depth from each other, corresponding to its location in 3D space (e.g., the gate is in front of the building and trees). <br>
        We propose OIS: order-aware interactive segmentation, to explicitly integrate this <span style="color: orange; font-weight:bold">missing relative depth information</span> into 2D interactive segmentation.
      </h2>
    </div>
  </div>
</section>






    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Interactive segmentation aims to accurately segment target objects with minimal user interactions. However, current methods often fail to accurately separate target objects from the background, due to a limited understanding of order, the relative depth between objects in a scene. To address this issue, we propose OIS: order-aware interactive segmentation, where we explicitly encode the relative depth between objects into order maps. We introduce a novel order-aware attention, where the order maps seamlessly guide the user interactions (in the form of clicks) to attend to the image features. We further present an object-aware attention module to incorporate a strong object-level understanding to better differentiate objects with similar order. Our approach allows both dense and sparse integration of user clicks, enhancing both accuracy and efficiency as compared to prior works. Experimental results demonstrate that OIS achieves state-of-the-art performance, improving mIoU after one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset as compared to the previous state-of-the-art SegNext, while also doubling inference speed compared to current leading methods. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">


    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Motivation. -->
        <h2 class="title is-3">Method</h2>

        <div class="subtitle has-text-justified">
          <p>
            By calculating the relative depth between each point in the image and the user prompt's location, we create <span style="color: orange; font-weight:bold">order map</span>, to effectively separate the prompt-selected object with others.
          </p>
        </div>

        <div class="container is-max-desktop">
          <div class="hero-body">
              </p>
              <img class="img-responsive" src="order.png">
              <p class="text-justify">
            <h2 align="left">
              <div class="subtitle has-text-justified">
                <p>With the help of order map, it is easy to use prompt click to separate objects. In (c), the foreground frame is fully separated from the background, and in (d), the background ball is clearly distinguished with just a negative click. (Red dots are positive clicks, blue dots are negative clicks, darker means closer to prompt-selected object, while lighter areas are farther to prompt-selected object)
                </p>
              </div>
             
            </h2>
          </div>
        </div>
</section>
<section class="section">
  <div class="container is-max-desktop">


    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Motivation. -->
        <h2 class="title is-3">Results</h2>

        <div class="subtitle has-text-justified">
          <p>
            Here, we present the qualitative comparison results of current state-of-the-art SegNext and OIS (Ours).
          </p>
        </div>

        <div class="subtitle has-text-justified">
          <p>
            One Click Results:
          </p>
        </div>

        <div class="image-container">
          <img class="img-responsive" src="oneclick.png" alt="one click">
        </div>
        <div class="image-container">
          <img class="img-responsive" src="oneclick2.png" alt="one click">
        </div>
        <div class="image-container">
          <img class="img-responsive" src="oneclick3.png" alt="one click">
        </div>                

        <div class="subtitle has-text-justified">
          <p>
            Multi-clicks Results:
          </p>
        </div>

        <div class="image-container">
          <img class="img-responsive" src="multiclick.png" alt="multi click">
        </div>        
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
        <pre><code>@article{wang2023gazesam,
          title={GazeSAM: What You See is What You Segment},
          author={Wang, Bin and Aboah, Armstrong and Zhang, Zheyuan and Bagci, Ulas},
          journal={arXiv preprint arXiv:2304.13844},
          year={2023}
        }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://openreview.net/pdf?id=hJ5DREWdjs">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/ukaukaaaa/GazeSAM" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://jerryxu.net/GroupViT">GroupViT</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>